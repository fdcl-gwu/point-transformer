{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation of Point Transformer modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gazebo Data Structure\n",
    "05.02.2025 \n",
    "- 9988 clouds w/poses\n",
    "- gazebo_pc_record_full_12_42_labeled_1024\n",
    "    - clouds/\n",
    "    - poses/\n",
    "\n",
    "    Up/Downsampling used:\n",
    "    - clouds with < 1024: random duplication with slight gaussian noise\n",
    "    - clouds with > 1024: FPS (takes a long time)\n",
    "    - not removing any clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. train_pose.py\n",
    "Dataloader: SimNet or ScanNet\n",
    "\n",
    "#### 2. pointtransformer_pose.py\n",
    "```self.radius = 1.5```\n",
    "```input_dim```: change based on number of one-hot encoded classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform=False means our .ply files are already downsampled to 1024 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Translation-only prediction: _t.py\n",
    "- use train_pose_t.py, test_pose_t.py, and pointtransformer_pose_t.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training+Inference Process: \n",
    "0. ```$ conda activate point-transformer```\n",
    "1. train_pose.py\n",
    "    - pointtransformer_pose.py: contains PoseLoss()\n",
    "        - quaternion_to_rotation_matrix() assumes wxyz format as of 12.02.25\n",
    "2. test_pose.py\n",
    "    - manually specify best model saved during training to use for inference\n",
    "    - writes results as .json in XYZW format\n",
    "3. overlay_pose_estimate.py (MAC)\n",
    "    - reads from json and for each cloud file we used in training set (which the .json stores the pose prediction for), overlays the cloud on the stl model for visual evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation+Improvements: 12.02.25\n",
    "- Rotate cloud by prediction, then see how much they overlap (using some metric like average distance of model points to transformed cloud (AD/ADS))\n",
    "- Need more input (number of scans, labels, etc)?\n",
    "- Use the ground truth model somehow? i.e. dense ship cloud?\n",
    "- Experiment with MSG radius\n",
    "- Data augmentation beyond sensor pose variation\n",
    "- Examine what the Point Transformer portion learns. How is it representing scans?\n",
    "- Study how point distribution affects prediction (stern deck is denser than port, for eg.)\n",
    "- Try maybe an ICP approach for refinement?\n",
    "    - For this, first plot the input cloud in the space that the model reads it in. The idea is we want to see whether the raw clouds are \"perturbed\" enough by default, (since they're in the sensor frame), or whether the model actually does learn most of the rotation in the SE3 space.\n",
    "- Big IDEA for now: review how the input scans are represneted to the model, how+what does it learn about the scan that lets it be roughly transformed \"correctly\" to the ship in pyvista.\n",
    "- Try this with the 4.0 val error model. How much do more epochs improve the predictions?\n",
    "\n",
    "#### Initial Observations:\n",
    "- Is translation is struggling more than rotation? (either too low or too high)\n",
    "- 008054.txt is quite off in the yaw direction\n",
    "\n",
    "#### 13.02:\n",
    "- [ ] Predict position first\n",
    "    - Don't forget about unit sphere scaling + centroid and how that affects it!\n",
    "    - read more about decoupling rot and trans. does this allow the model to learn each better? i.e. would I try to learn rotation around origin? This again fits into the unit sphere situation i have right now.\n",
    "- [ ] visualize top K \n",
    "- [ ] quickly NEXT, train by concatenating scale, centroid?\n",
    "- [ ] try quaternion loss\n",
    "- [ ] NEXT, try training without unit-sphere scaling (just centroid)\n",
    "    - might need to experiment with adjusting radii if not unit-sphere, or concatenating global scale? think...\n",
    "- [ ] try with trained data\n",
    "- [ ] use ICP (geometric registration problem)? after transformer gets approximate pose\n",
    "    - \"In order to improve the registration perfor-\n",
    "mance, features on point clouds are also introduced for\n",
    "matching. \"\n",
    "- [ ] loss function predicting angle-axis, but does this cause issue with conversion?\n",
    "    - try different loss function (see GDR-Net), other parameterization besides angle axis\n",
    "- [ ] then try keypoint prediction\n",
    "    - plot the predicted keypoints after unit sphere scaling applied to them to ensure \n",
    "\n",
    "Question: should validation be used durign training and fed back? Or just printed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUATERNION FORMAT: WXYZ or XYZW: 12.02.25\n",
    "- gt stored as WXYZ quaterion\n",
    "- model learns angle-axis\n",
    "    - within loss function, its converted to rotation matrix to compare with ground truth (also converted to matrix)\n",
    "- NOTE: during inference (test_pose.py), we write pred_quat as XYZW to results.json!!\n",
    "    -  now writes ground truth gt_quat to be XYZW to results.json\n",
    "- on Mac, in overlay_pose_estimate.py, reads from results.json which has all XYZW format.\n",
    "\n",
    "<!-- TODO: ground truth should be as XYZW too -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
