{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation of Point Transformer modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gazebo Data Structure\n",
    "05.02.2025 \n",
    "- 9988 clouds w/poses\n",
    "- gazebo_pc_record_full_12_42_labeled_1024\n",
    "    - clouds/\n",
    "    - poses/\n",
    "\n",
    "    Up/Downsampling used:\n",
    "    - clouds with < 1024: random duplication with slight gaussian noise\n",
    "    - clouds with > 1024: FPS (takes a long time)\n",
    "    - not removing any clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. train_pose.py\n",
    "Dataloader: SimNet or ScanNet\n",
    "\n",
    "#### 2. pointtransformer_pose.py\n",
    "```self.radius = 1.5```\n",
    "```input_dim```: change based on number of one-hot encoded classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform=False means our .ply files are already downsampled to 1024 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Translation-only prediction: _t.py\n",
    "- use train_pose_t.py, test_pose_t.py, and pointtransformer_pose_t.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training+Inference Process: \n",
    "0. ```$ conda activate point-transformer```\n",
    "1. train_pose.py\n",
    "    - pointtransformer_pose.py: contains PoseLoss()\n",
    "        - quaternion_to_rotation_matrix() assumes wxyz format as of 12.02.25\n",
    "2. test_pose.py\n",
    "    - manually specify best model saved during training to use for inference\n",
    "    - writes results as .json in XYZW format\n",
    "3. overlay_pose_estimate.py (MAC)\n",
    "    - reads from json and for each cloud file we used in training set (which the .json stores the pose prediction for), overlays the cloud on the stl model for visual evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation+Improvements:\n",
    "- Rotate cloud by prediction, then see how much they overlap (using some metric like average distance of model points to transformed cloud (AD/ADS))\n",
    "- Need more input (number of scans, labels, etc)?\n",
    "- Use the ground truth model somehow? i.e. dense ship cloud?\n",
    "- Experiment with MSG radius\n",
    "- Data augmentation beyond sensor pose variation\n",
    "- Examine what the Point Transformer portion learns. How is it representing scans?\n",
    "- Study how point distribution affects prediction (stern deck is denser than port, for eg.)\n",
    "- Try maybe an ICP approach for refinement?\n",
    "    - For this, first plot the input cloud in the space that the model reads it in. The idea is we want to see whether the raw clouds are \"perturbed\" enough by default, (since they're in the sensor frame), or whether the model actually does learn most of the rotation in the SE3 space.\n",
    "- Big IDEA for now: review how the input scans are represneted to the model, how+what does it learn about the scan that lets it be roughly transformed \"correctly\" to the ship in pyvista.\n",
    "- Try this with the 4.0 val error model. How much do more epochs improve the predictions?\n",
    "\n",
    "#### Initial Observations:\n",
    "- Is translation is struggling more than rotation? (either too low or too high)\n",
    "- 008054.txt is quite off in the yaw direction\n",
    "\n",
    "#### 13.02:\n",
    "- [x] Predict position first\n",
    "    - Don't forget about unit sphere scaling + centroid and how that affects it!\n",
    "- [ ] read more about decoupling rot and trans. does this allow the model to learn each better? i.e. would I try to learn rotation around origin? This again fits into the unit sphere situation i have right now.\n",
    "- [ ] Even distribution of scans in dataset (i.e. reduce bias towards close scans)\n",
    "    - Also means limit range of scan (i.e max furthest point from sensor)\n",
    "- [ ] visualize top K \n",
    "- [x] quickly NEXT, train by concatenating scale, centroid?\n",
    "    - no real impact on result\n",
    "- [ ] try quaternion loss\n",
    "- [ ] NEXT, try training without unit-sphere scaling (just centroid)\n",
    "    - might need to experiment with adjusting radii if not unit-sphere, or concatenating global scale? think...\n",
    "- [x] try with trained data (not much of a difference)\n",
    "- [ ] use ICP (geometric registration problem)? after transformer gets approximate pose\n",
    "    - \"In order to improve the registration perfor-\n",
    "mance, features on point clouds are also introduced for\n",
    "matching. \"\n",
    "- [ ] loss function predicting angle-axis, but does this cause issue with conversion?\n",
    "    - try different loss function (see GDR-Net), other parameterization besides angle axis\n",
    "- [ ] then try keypoint prediction\n",
    "    - plot the predicted keypoints after unit sphere scaling applied to them to ensure \n",
    "\n",
    "Question: should validation be used durign training and fed back? Or just printed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUATERNION FORMAT: WXYZ or XYZW: 12.02.25\n",
    "- gt stored as WXYZ quaterion\n",
    "- model learns angle-axis\n",
    "    - within loss function, its converted to rotation matrix to compare with ground truth (also converted to matrix)\n",
    "- NOTE: during inference (test_pose.py), we write pred_quat as XYZW to results.json!!\n",
    "    -  now writes ground truth gt_quat to be XYZW to results.json\n",
    "- on Mac, in overlay_pose_estimate.py, reads from results.json which has all XYZW format.\n",
    "\n",
    "<!-- TODO: ground truth should be as XYZW too -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "- Add plotting information to thesis document. Explain how i'm evaluating the results.\n",
    "\n",
    "- Tried directly regressing pose (translation resitual), but still not perfect. Before working with more/better dataset and dataset fine tuning, keep it how it is, and try\n",
    "\n",
    "    a) \n",
    "\n",
    "        - TRY THIS:Once predicting R,t, apply it to cloud and compute distance to gound truth model. use this as part of the loss (where alignment has lowest loss)\n",
    "        - this way the model also can compare the actual cloud error vs just the rotation error. Since we keep model in unnormalized space, this may be a work around.\n",
    "    b)\n",
    "\n",
    "        - TRY THIS: use PNP algorithm for predicting key points and then compute pose from those keypoints.\n",
    "\n",
    "    c)  - Still try to get centroid only (i.e. no scale) to work \n",
    "\n",
    "Once the \"Transformer\" is as good as possible, then:\n",
    "    a) Explore pose refinement (some ICP with ground truth ship model (since we have it for CAD and real ship))\n",
    "\n",
    "- keep everyting in normalized for training, but maybe give it some information in addition to this info?? Just appending before pose head didn't do anything.\n",
    "    - IDEA: can we make the \"label\" be the scale? So each scan cloud has a 4th entry which is the scale computed for that cloud? But then again, that's the same issue as not having a normalized input vector to begin with (i.e. all numbers are [-1,1] but 4th is large)\n",
    "- Clip scan cloud ranges, and experiment with query_ball_point radius (in case < nsample nearest points are found) \n",
    "- Think what ways we can use the Point Transformer output? It's basically just a good cloud feature representation.\n",
    "- Explore pose refinement (some ICP with ground truth ship model (since we have it for CAD and real ship))\n",
    "\n",
    "- ONLY THEN: try real data\n",
    "    - will allow model to learn with real noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Approach - Feb 21\n",
    "1. Try different ICP models between scan cloud and ground truth point cloud at inference time.\n",
    "    - [x] GICP (Open3D)\n",
    "    - [ ] Go-ICP (global one)\n",
    "    - [ ] small_gicp\n",
    "2. Incorporate alignment error into model training architecture \n",
    "    - [ ] Apply R,t to cloud and compute alignment error: (_D. 3D Rotation Regression. PoseCNN paper_)\n",
    "        - [ ] error using point-to-point. Compute error btw both clouds and add it to the loss function.\n",
    "        - [ ] error comparing scan and global feature. Compute feature vector of transformed cloud and ground truth cloud, where alignment of the two have lower loss (key: what does the \"feature difference\" look like?).\n",
    "    - [ ] If not, try PointNetLK to iteratively refine R,t instead of predicting PoseLoss\n",
    "    - See Deep Closest Point, PCRNet, Go-ICP, DeepVCP/DeepCLR,\n",
    "    - Explore whether point-to-point or feature-to-feature is better (since we're not registering identical scans)\n",
    "3. Other ideas:\n",
    "    - __Resolve scale problem:__\n",
    "        - concatenate scale to each point in cloud. Challenge: not normalized either\n",
    "            - Sol: normalize scale dataset wide? Yes, but save the mean-std scales computed on the trianing dataset to apply to the scan at inference.\n",
    "        - make scale prediction MLP in additon to R,t (_Disentangled 6D Pose Loss.GDR-Net paper_)\n",
    "            - doesn't force the feature extraction layers to also learn scale, since its all tied into the final estimate.\n",
    "        - maybe alignment/chamfer solves it. Challenge: may be costly to compute.\n",
    "\n",
    "\n",
    "The BIG IDEA: we keep Point Transformer in unit sphere space since it can learn features fairly well, but then we a) improve refinement to be robust (deep learning registration options) and/or b) let the model learn with alignment error (in some combination of the two). __Features are more robust than points__\n",
    "\n",
    "#### QUICK IDEAS \n",
    "25.Feb:\n",
    "- add scale to 4th col point feature (with avg normalized across dataset)\n",
    "- try alignment loss function (option 1, option 2)\n",
    "- \"intelligent\" ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - Point Transformer\n",
    "\n",
    "#### NOTE: 16.02-19.02 Using squared L2 norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ BEST RUN ------------------------\n",
    "_2025-02-20_21-50_:\n",
    "```\n",
    "{'num_points': 1024, 'batch_size': 11, 'use_labels': False, 'optimizer': 'RangerVA', 'lr': 0.001, 'decay_rate': 1e-06, 'epochs': 60, 'dropout': 0.4, 'M': 4, 'K': 64, 'd_m': 512, 'alpha': 10, 'beta': 1, 'radius_max_points': 32, 'radius': 0.2, 'unit_sphere': True}\n",
    "\n",
    "Using L2 Norm, 12 Dataset\n",
    "```\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 16.02.25 (1): add scale term to pose head input (_t only)\n",
    "NOTE!!: initially forgot to multiply `predicted_translation` by `scale`\n",
    "\n",
    "Result dir: pose_estimation/2025-02-16_15-59\n",
    "Observation: loss not much better original approach, \n",
    "\n",
    "Changes: \n",
    "\n",
    "- pointtransformer_pose_t.py\n",
    "    ```\n",
    "    self.translation_mlp = nn.Sequential(\n",
    "    nn.Linear(dim_flatten+1, 512),\n",
    "    ```\n",
    "\n",
    "- within PointTransformer.forward():\n",
    "    ```\n",
    "    # Flatten the feature vector for MLP heads\n",
    "    global_features = torch.flatten(embedding, start_dim=1)  # [B, dim_flatten]\n",
    "    if scale.dim() == 1:\n",
    "        scale = scale.unsqueeze(1)  # Expands shape from [B] to [B, 1]\n",
    "    # Predict translation residual (normalized space)\n",
    "    translation_input = torch.cat([global_features, scale], dim=1)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 16.02.25 (2): Hybrid - Only Centroid, No Scale (INCOMPLETE)\n",
    "\n",
    "\n",
    "Set config.unit_sphere = False. The issue currently is that conv2d dimensions for the ball query search aren't working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (1): Regular training run\n",
    "Details: \n",
    "- ```predicted_translation = predicted_translation_residual * scale + centroid``` applied to model output in PoseLoss()\n",
    "- 'alpha': 20, 'beta': 2, 'radius_max_points': 32, 'radius': 0.2\n",
    "\n",
    "Result dir: 2025-02-19_15-10\n",
    "\n",
    "Observation: decent R, t prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (2): Learning absolute translation (not residual)\n",
    "Details: ```predicted_translation = predicted_translation_residual```\n",
    "\n",
    "Result dir: 2025-02-19_15-20\n",
    "\n",
    "Observation: terrible translation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (3): Regular training run\n",
    "Details: 'radius_max_points': 32, 'radius': 0.1,\n",
    "\n",
    "Result dir: 2025-02-19_20-57 (100 epoch: 2025-02-19_23-03, 'radius': 0.2)\n",
    "\n",
    "Observation: regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (4): Regular training run SimNet2\n",
    "Details: same as 2025-02-19_20-57, except on SimNet2 (i.e. 15.41 dataset)\n",
    "\n",
    "Result dir: 2025-02-19_21-27 (100 epoch: 2025-02-19_23-05, 'radius': 0.2)\n",
    "\n",
    "Observation: better than fewer epochs, but still not much different... a bit worse visually than 12.42 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 21.02.25 (1/2): L2 Loss (not squared L2 Loss)\n",
    "Details: corrected L2 Loss with two different learning rates. Note, training uses lr scheduler.\n",
    "\n",
    "Result dir: 2025-02-20_21-49 (for 'lr': 0.0005), 2025-02-20_21-50 (for 'lr': 0.001)\n",
    "\n",
    "Observation: translation a bit better, but rotation not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 21.02.25 (3):'radius_max_points': 16,'radius': 0.1\n",
    "Details: tried using default params as specified in papers\n",
    "\n",
    "Result dir: 2025-02-21_12-33\n",
    "\n",
    "Observation: quite a lot worse than 32,'radius': 0.2. Likely a property of the clouds that are captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 25.02.25 (1):'lr scheduler step_size': 15, dataset = merged 12,15 (SimNet15)\n",
    "Details: reduced lr scheduler step size\n",
    "\n",
    "Dataset: using merged datasets `gazebo_pc_record_full_12_42_1024` and `gazebo_pc_record_full_15_41_1024`.\n",
    "\n",
    "Result dir: 2025-02-25_14-01 (on MAC, 02-25_16-35)\n",
    "\n",
    "Observation: similar to best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 25.02.25 (2):Scale MLP layer\n",
    "Dataset: 12\n",
    "\n",
    "Details: lr scheduler step size=15, added scale_mlp(),  and model now predicts scale \n",
    "\n",
    "Changes:\n",
    "- In PoseLoss()\n",
    "    ```\n",
    "        loss_scale = F.mse_loss(pred_scale, gt_scale)\n",
    "        total_loss = (self.alpha * loss_t) + (self.beta * loss_r) + loss_scale\n",
    "    ```\n",
    "\n",
    "- in PointTransformer.forward()\n",
    "    ```\n",
    "        predicted_scale = self.scale_mlp(global_features)\n",
    "        scale = scale.unsqueeze(1)  # Expands shape from (B,) to (B,1)\n",
    "        predicted_translation = predicted_translation_residual * predicted_scale + centroid\n",
    "\n",
    "    ```\n",
    "\n",
    "Result dir: 2025-02-25_13-35 (on MAC, 02-25_16-00)\n",
    "\n",
    "Observation: similar to best\n",
    "TODO: try with normalized scale instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 27.02.25 (1): Larger+more MLP layers\n",
    "Dataset: 12\n",
    "\n",
    "Details: lr scheduler step size=15,  made pose MLP heads start at 4096, but shoudl also try 1024\n",
    "\n",
    "Result dir: \n",
    "- 4096:\n",
    "    - 2025-02-27_12-06\n",
    "- 1024:\n",
    "    - 2025-02-27_12-11 (on MAC: 2025-02-27_13-32)\n",
    "\n",
    "Observation: slightly worse than 16-00 and 16-35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (1): New Dataset (far, normal)\n",
    "Dataset: SimNet_far\n",
    "\n",
    "Details: lidar samples=512, using truncated normal dist.\n",
    "\n",
    "Result dir: \n",
    "- 2025-03-02_14-49\n",
    "\n",
    "Observation: No real difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (2): New Dataset (close, normal)\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: lidar samples=512, using truncated normal dist. reduced theta for C points so we don't cover side of ship as much. \n",
    "\n",
    "Result dir: \n",
    "- 2025-03-03_12-02 (60 epochs) (on MAC: 2025-03-03_13-48)\n",
    "- 2025-03-03_14-21 (120 epochs) (on MAC: 2025-03-03_19-35)\n",
    "\n",
    "Observation: \n",
    "- 60 epochs: roll usually better, since flight deck is large flat surface (easy to align)\n",
    "    - GICP using closest point, which doesn't help translation alignment (14/500)\n",
    "    - bad one: 13/500\n",
    "- 120 epochs: marginally better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (3): Merged Dataset (close+far, normal)\n",
    "Dataset: SimNet_close + SimNet_far\n",
    "\n",
    "Details:  \n",
    "\n",
    "Result dir: \n",
    "- 2025-03-03_14-23 (on MAC: 2025-03-04_09-54)\n",
    "\n",
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - KPLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 14.03.25 (1): first key point prediction attempt (SCALE??). \n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: DataLoader keypoints global scale, PointTransformer forward() *scale+centroid when return pred_keypoints\n",
    "\n",
    "Result dir: \n",
    "- 2025-03-14_15-23 (on MAC: 2025-03-14_16-06)\n",
    "\n",
    "Observation: best so far, can't reproduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 14.03.25 (2): global scale for keypoints\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: DataLoader keypoints -centroid/scale. PointTransformer forward() *scale+centroid before loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-03-14_16-53 (on MAC: )\n",
    "\n",
    "Observation: not decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 14.03.25 (3): unit scale for keypoints\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: DataLoader normalized keypoints, points. PointTransformer forward() doesn't scale back before loss computation (i.e. stay in unit scale)\n",
    "\n",
    "Result dir: \n",
    "-  (on MAC: )\n",
    "\n",
    "Observation: not decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.03.25 (1): unit scale for keypoints (actually correct)\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: Not using scale, centroid. all normalized for now.\n",
    "\n",
    "Result dir: \n",
    "-  (on MAC: 2025-03-19_11-04)\n",
    "\n",
    "Observation: shape preserved interestingly, not pose alignment??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.03.25 (2): unit scale for keypoints, stern+doghouse kp\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: loss function with 1 1 0.3 1.2 2.5 for each of 5 terms (with weird \"size loss\" term)\n",
    "\n",
    "Result dir: \n",
    "-  2025-03-19_12-28 (on MAC: 2025-03-19_14-37)\n",
    "\n",
    "Observation: best keypoint prediciton so far, but not rectangular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 23.03.25 (1): just MLP for keypoint prediction, different weights. doghouse kp\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: total loss around 0.002\n",
    "- self.alpha = 1  # Class (cross entropy) scaling (not used really)\n",
    "- self.beta = 3   # Smooth L1 scaling for regression\n",
    "- self.delta = 5 # rotation loss scaling\n",
    "- self.epsilon = 6 # center loss scaling\n",
    "\n",
    "Result dir: \n",
    "-  2025-03-23_14-07 (on MAC: 2025-03-23_14-43)\n",
    "\n",
    "Observation: lower with huber loss (smooth_l1_loss) than MSE loss\n",
    "- rot_loss=0.000x, but keypoint and center loss both around 0.002. Predictions are \"around\" the doghouse, but not good at all, or rectangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPLoss: \"This is the correct architectural match for your problem. The classification-style output (flatten + MLP) is fundamentally mismatched to keypoint prediction because it discards spatial structure that the Point Transformer is designed to preserve.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - Decoder Attention Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 02.04.25 (1): keypoints predicted using Decoder idea (with per-point learning like in partseg)\n",
    "Dataset: SimNet_close, keypoints_st_dg_few\n",
    "\n",
    "Details:\n",
    "    'alpha': 2,\n",
    "    'beta': 5,\n",
    "    'gamma': 3,\n",
    "    'delta': 1,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "-  2025-04-02_17-11 (on MAC: 2025-04-02_19-14)\n",
    "- TODO: accidently deleted 2025-04-02_17-11 result dir. Rerun if desired.\n",
    "\n",
    "Observation: Best keypoint prediction yet. Box shape + orientation very close to ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.04.25 (1): keypoints predicted using Decoder idea (with per-point learning like in partseg). Same as above, but with 140 keypoints\n",
    "Dataset: SimNet_close, keypoints (140 doghouse)\n",
    "\n",
    "Details:\n",
    "    'alpha': 2,\n",
    "    'beta': 5,\n",
    "    'gamma': 3,\n",
    "    'delta': 1,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "-  2025-04-03_11-34 (on MAC: 2025-04-03_13-35)\n",
    "\n",
    "Observation: Orientation not as good as 40 prediction, but shape preserved fairly well. Can play with hyperparams? But I think there are just too many points, and the doghouse may not be well represented in the scans (compared to more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.04.25 (2): same as Test 02.04.25 (1), but with different config hyperparams \n",
    "Dataset: SimNet_close, keypoints (140 doghouse)\n",
    "\n",
    "Details: \n",
    "    'alpha': 4,\n",
    "    'beta': 6,\n",
    "    'gamma': 5,\n",
    "    'delta': 1,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "-  2025-04-03_11-58 (on MAC:)\n",
    "\n",
    "Observation: simiar to first, but a bit worse? TBD\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDO FIRST (i.e. Test 02.04.25 (1))\n",
    "\n",
    "except following changes:\n",
    "step_size=30\n",
    "100 epochs\n",
    "\n",
    "Result Dir: 2025-04-03_13-56 (On MAC: 2025-04-04_15-19)\n",
    "\n",
    "Observation: BEST SO FAR (Keypoint alignment when reprojected in plot_kp_pred.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIX: z coordinate of cad_keypoints now moved -0.5m\n",
    "Issue was that the points in ```stl_keypoints_cfg``` (which is parsed by keypoint_label_points.py) isn't shifted down. Thus, the resulting ```cad_keypoints_40_cfg_st_dg_few.txt``` was also not shifted down (to match the gazebo model being 0.5m below the z=0 plane where the sensor origin is set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 08.04.25 (1): with pose_loss and centroid loss (new)\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "    'alpha': 2,\n",
    "    'beta': 3,\n",
    "    'gamma': 5,\n",
    "    'delta': 0.0005,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "NOTE: __beta is now centroid Loss__\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-08_16-41 (on MAC: 2025-04-08_17-28)\n",
    "\n",
    "Observation: rot perfect, translation not\n",
    "\n",
    "#### Attempt 2:\n",
    "            'alpha': 2.5,\n",
    "            'beta': 10,\n",
    "            'gamma': 3.5,\n",
    "            'delta': 0.005,\n",
    "            'epsilon': 1,\n",
    "\n",
    "Result Dir: 2025-04-08_17-49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 08.04.25 (2): with pose_loss and procrustes loss (old)\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "    'alpha': 2,\n",
    "    'beta': 4,\n",
    "    'gamma': 3.5,\n",
    "    'delta': 0.0005,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-08_16-45 (on MAC: 2025-04-08_17-32)\n",
    "\n",
    "Observation:\n",
    "\n",
    "GitHub: see commit mentioning procrustes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 09.04.25 (1): with pose_loss and centroid loss, corected cad kp frames (see Pose Estimation from LiDAR chat)\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "    'alpha': 2.5,\n",
    "    'beta': 10,\n",
    "    'gamma': 3.5,\n",
    "    'delta': 0.005,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-09_14-40 (on MAC: )\n",
    "\n",
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 09.04.25 (2): like (1), but without pose_loss\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "    'alpha': 2.5,\n",
    "    'beta': 10,\n",
    "    'gamma': 3.5,\n",
    "    'delta': 0.0,\n",
    "    'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-09_14-58 (on MAC: )\n",
    "\n",
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.04.25 (1): kabsch, centroid loss\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "            'alpha': 4,\n",
    "            'beta': 2,\n",
    "            'gamma': 2,\n",
    "            'delta': 0,\n",
    "            'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-10_18-09 (on MAC: 2025-04-10_19-48)\n",
    "\n",
    "Observation: bad R, t (issue after keypoints predicted, but keypoints look close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.04.25 (2): kabsch, procrustes loss (no pose_loss delta)\n",
    "Dataset: SimNet_close, keypoints_st_dg_few (40)\n",
    "\n",
    "Details: \n",
    "            'alpha': 2,\n",
    "            'beta': 4,\n",
    "            'gamma': 5,\n",
    "            'delta': 0.0,\n",
    "            'epsilon': 1,\n",
    "alignment loss: smooth_l1_loss\n",
    "\n",
    "Result dir: \n",
    "- 2025-04-10_19-57 (on MAC: 2025-04-11_11-26)\n",
    "\n",
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R,t pred in results.json working\n",
    "Everything working training (corrected Kabsch for translation)\n",
    "\n",
    "            'alpha': 2,\n",
    "            'beta': 4,\n",
    "            'gamma': 5,\n",
    "            'delta': 0.0001,\n",
    "            'epsilon': 1,\n",
    "2025-04-11_19-24 (On MAC: 2025-04-11_20-31)\n",
    "Training crashed at epoch 19 (val loss went to nan)\n",
    "\n",
    "            'alpha': 5,\n",
    "            'beta': 4,\n",
    "            'gamma': 7,\n",
    "            'delta': 0.0,\n",
    "            'epsilon': 1,\n",
    "2025-04-11_19-50 (On MAC: 2025-04-13_12-27 (NO GICP) AND 2025-04-16_13-57 (WITH GICP)) \n",
    "__BEST ONE__\n",
    "\n",
    "MAKE SURE TO UPDATE test_pose.py hyperparams based on those used for the selected best_model.pth is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Dataset: ScanNet\n",
    "2025-05-05_16-44 (inference ON MAC: 2025-05-06_15-31)\n",
    "- using simple keypoint + pairwise loss\n",
    "- code saved under train_pose_real.py\n",
    "\n",
    "### Sim Dataset: SimNet_close\n",
    "2025-05-05_15-52\n",
    "- using simple keypoints + pairwise loss\n",
    "\n",
    "### TRAINED ON SIMULATED, INFERENCE ON REAL\n",
    "On MAC: pose_est_inference/2025-05-08_11-56 (using 2025-05-05_15-52/best_model.pth for test_pose.py, but ScanNet Dataset)\n",
    "- Observations: \n",
    "    - Only chanhged .pth for test_pose.py. When running on Mac (```results/trans_rot_error.py``` and ```results/viz_results_json```, only changed results.json file), results were worse. However, I didn't do any pre-training, checking whether I need to change anything else (like using STL model somewhere instead of yp cloud?), etc. TODO for later if he wants me to show that we can train with simulation ENTIRELY, and run inference on real scans (which also have noise that simulated training wasn't exposed to).\n",
    "    - With lots of noise like water and people, the model struggles since those points aren't in Simluated training data. Since I don't do any localization of the ship in the environment for now, this is likely where things fail. \n",
    "    - Can also try augmenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING READY TO WRITE:\n",
    "ON MAC\n",
    "- viz_results_json.py: will plot the predicted and refined R,t using the results.json file as input for all points, scale/centroid, keypoints and predicted/refined poses. Basically visualies the model inference results\n",
    "    - FOR THESIS, change ```init_T_target_source``` to be ```np.eye(4)``` to show that initial guess from PoinTransformer is crutial. basically shows incorrect alignments of GICP.\n",
    "- sim_model_gt_w_noise.py: \n",
    "    - manually check single scan (in trans_error/ directory)\n",
    "    - Note that \"w_noise\" isn't really applicatble anymore in this file. originally, I added noise to the gt keypoints since I didn't have predictions yet. Now, however, I now read directly from the results.json copied from the server after test_pose.py runs.\n",
    "        - Global methods all take very long (because it never reaches the converged pose, especialy for sparse side scans), so fast keypoint prediciton and refinement is best.\n",
    "\n",
    "### small_gicp obsevations:\n",
    "    - on MAC, takes 0.03sec\n",
    "    - side scans fail worse with keypoint prediction because doghouse/stern (whcih are the keupoint defined regions) don't appear in the scan much. As such, it has little to work with.\n",
    "        - direct pose regression may not have been much worse, but it's hard to visualize/understand/and may be more sensitive to noise? TEST THIS OUT FIRST by going back in commit history? Or just check the saved model\n",
    "            - check if keypoints has better translation error (x direction for example)? I notice that even bad keypoint transformer prediction seems to align doghouse/net better\n",
    "\n",
    "### Next Steps:\n",
    "- try Simnet Close + Far datasets (need to generate keypoints for that again)\n",
    "\n",
    "BIG DIFFERENCE SIM vs REAL Dataset: ship is 0.5m down in simualted\n",
    "\n",
    "### FUTURE WORK:\n",
    "- more keypoints (i.e. more ship regions). \n",
    "    - add classification to predict ship region in scan and THEN predict keypoints. This way model can predict specific ship keypoints based just on \"what it sees\".\n",
    "- better real-world dataset (DLIO from UCLA)\n",
    "    - concerned about real-world ground truth keypoint positions... need to see whether too much error is bad, or if the error still works with the refinement. mainly a dataset issue I presume.\n",
    "    - HARD IDEA: instead of fixed keypoints, maybe just use \"what's already there\" and somehow make them dynamic keypoints? i.e. ground truths are hard and may not align too well with . Should use IMU instead? The initial \"alignment\" error in my manual frame correction continues for the remaining of the whole real-world flight.\n",
    "- TO HANDLE REAL_WORLD NOISE, learn to segment the ship (i.e. isolate the ship from the scan better, then apply point-transformer to those points (like Frustum points paper does))\n",
    "- LATER TODO: explore KeypointDETR paper which can get keypooints for other, \"unkknown\" ship classes?\n",
    "SEMI-QUICK IMPROVEMENT: make sure all scans are \"similarly scaled\"... meaning no super far outlier (like single point at bow of ship) that skews the centroid and thus the SortNet query ball radius neighborhood search.\n",
    "\n",
    "- challenges: normalizing for point transformer kills \"global\" translation scale and model has to infer it solely from scan cloud... which it can't do perfectly.keypoints are more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint Training Instructions:\n",
    "Current Keypoint Datasets:\n",
    "- 40 keypoints\n",
    "    - Details: 20 stern, 20 doghouse\n",
    "    -Per-scan keypoints in sensor frame: \n",
    "    ```data/SimNet_close/gazebo_pc_record_os0_rev06-32_r8_seed42_r_C_normal_mu6_std5_filt1.2_thetaBC-ov3_thetaBF-ov5_rf4-7_labeled_1024/keypoints_st_dg_few/```\n",
    "    - CAD keypoints in ship frame: ```data/cad_keypoints_40_cfg_st_dg_few.txt```\n",
    "- 140 keypoints\n",
    "    - Details: 140 doghouse\n",
    "    - Per-scan keypoints in sensor frame:\n",
    "    ```/home/karlsimon/point-transformer/data/SimNet_close/gazebo_pc_record_os0_rev06-32_r8_seed42_r_C_normal_mu6_std5_filt1.2_thetaBC-ov3_thetaBF-ov5_rf4-7_labeled_1024/keypoints```\n",
    "    - CAD keypoints in ship frame: ```data/cad_keypoints_140```\n",
    "- To change the keypoints used for training\n",
    "\n",
    "Code Changes:\n",
    "train_pose.py/test_pose.py:\n",
    "- config['num_keypoints']\n",
    "- cad_keypoint_file\n",
    "\n",
    "SimNetDataLoader:\n",
    "- keypoints_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint Training Ideas:\n",
    "- Use dimensions from the ground truth point cloud somehow as part of the keypoint losses? I.e. examine again how the keypoints learned, and whether supervision from the ground truth model would help at all? See \"Keypoint Prediction for 6DoF\" Chat for history.\n",
    "\n",
    "- TODO: try with MSE instead of Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM-TO-REAL Training: see config.txt next to train results for hyperparams\n",
    "\n",
    "### Sim-To-Real Research Goal (I):\n",
    "- Train with SimNet simlation data, and test with ScanNet_train.\n",
    "\n",
    "Train: 2025-05-15_17-03 \n",
    "Test: (On MAC: 3 GICP iters in test_pose.py: On MAC: 2025-05-16_11-54)\n",
    "\n",
    "Test with ScanNet_test: On MAC: 2025-05-16_14-01\n",
    "\n",
    "### Sim+Real Training (II):\n",
    "Pre-train using (I) (i.e. just SimNet). Continue training with ScanNet_train (only 15% of training data used though). Then test with ScanNet_test.\n",
    "\n",
    "Pre-Train: 2025-05-16_13-26 (On MAC: 2025-05-16_13-59)\n",
    "\n",
    "### Real Training (III)\n",
    "Trained on ScanNet_train ONLY (no simulation). Tested on ScanNet_test.\n",
    "\n",
    "2025-05-15_17-14 (On MAC: 2025-05-16_12-24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS:\n",
    "\n",
    "21.03: add more keypoints, change weights of loss, try with only doghouse\n",
    "\n",
    "IDEA: fixed box of known size, and its vertices must be placed on the ship. The keypoints are contrained to be this size (how to define this contraint)??\n",
    "- somehow use the are of the box formed by the key points (IoU?) using a graph-based approach? This way the points that are predicted are contrained\n",
    "- The ball query still is picking up different features depending on whether the scan cloud is more spread out or not. when scaled and centered to unit sphere, i.e. 0.1m may correspond to different ranges in different scans.\n",
    "\n",
    "On MAC: stl_keypoints_increase.py will take ship configuration and add points between the \"sparse\" bounding boxes. This writes to ship_structure_subdivided.json, which can be copied to stl_keypoints_cfg (making sure to paste the json results after \"ship_structure = \" since the *_increase.py file writes to the json only starting with the '{')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - GICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ BEST RUN ------------------------\n",
    "\n",
    "#### Using _2025-02-20_21-50_ (i.e. 2025-02-21_12-22 on MAC after running test_pose.py):\n",
    "- Uses TransformationEstimationForGeneralizedICP with max_iteration=1\n",
    "\n",
    "- Under `model_ouput/2025-02-21_12-22`\n",
    "    - fitness_rmse_15_fit_rmse.txt and refined_results fitness_rmse_15_fit_rmse.json\n",
    "#### ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - Key Point Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding keypoints to SimNetDataLoader with one-hot label depending on the region that the keypoints enclose. Normalized the same way as the scan.\n",
    "\n",
    "Arhitecture: Point Transformer to learn features:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation Refinement: Two Training Strategies\n",
    "\n",
    "### Option 1: Point-Level Alignment (Chamfer Distance)\n",
    "#### Overview:\n",
    "- After predicting the pose (R, t), apply it to transform the scan cloud.\n",
    "- Compute alignment loss between the transformed scan and the ground truth ship cloud.\n",
    "- Loss penalizes misalignment, improving pose predictions over training.\n",
    "\n",
    "#### Steps:\n",
    "1. Extract scan features using Point Transformer.\n",
    "2. Predict pose (R, t) using Pose MLP.\n",
    "3. Apply the predicted transformation to the scan cloud.\n",
    "4. Compute Chamfer Distance between transformed scan and ship cloud.\n",
    "5. Backpropagate alignment loss to refine pose predictions.\n",
    "\n",
    "#### Loss Function:\n",
    "- Pose loss (direct R, t error).\n",
    "- Chamfer Distance loss (point-wise alignment error).\n",
    "- Total loss combines both.\n",
    "\n",
    "#### Key Benefits:\n",
    "- Simple to implement and integrates well into direct pose regression.\n",
    "- Improves translation prediction, especially with unit scaling issues.\n",
    "- Does not require a learned feature vector for the ship model.\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Feature-Based Alignment (Global Feature Comparison)\n",
    "#### Overview:\n",
    "- Instead of point-wise alignment, compare feature vectors between the transformed scan and the ship.\n",
    "- Forces the network to learn feature spaces where aligned scans and ships are similar.\n",
    "\n",
    "#### Steps:\n",
    "1. Extract scan features using Point Transformer.\n",
    "2. Predict pose (R, t) using Pose MLP.\n",
    "3. Apply the predicted transformation to the scan cloud.\n",
    "4. Extract a global feature vector from the transformed scan.\n",
    "5. Extract a fixed global feature vector for the ship cloud.\n",
    "6. Compute alignment loss based on feature similarity.\n",
    "7. Backpropagate to refine both feature learning and pose prediction.\n",
    "\n",
    "#### Loss Function:\n",
    "- Pose loss (direct R, t error).\n",
    "- Feature distance loss (L2 difference between transformed scan and ship features).\n",
    "- Total loss combines both.\n",
    "\n",
    "#### Key Benefits:\n",
    "- Helps the model generalize better by enforcing feature-based alignment.\n",
    "- Encourages the Point Transformer to extract features that naturally align after transformation.\n",
    "- Reduces the need for iterative refinement at inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Differences\n",
    "| Feature | Option 1: Chamfer Distance | Option 2: Feature Alignment |\n",
    "|---------|---------------------------|----------------------------|\n",
    "| **Alignment Type** | Point-wise (geometry-based) | Feature-space (embedding-based) |\n",
    "| **Ship Model Representation** | Fixed point cloud (no learned features) | Learned feature representation |\n",
    "| **Loss Supervision** | Chamfer Distance between point clouds | L2 difference between feature vectors |\n",
    "| **Computational Complexity** | Moderate | Higher due to feature extraction for ship model |\n",
    "| **Training Effect** | Directly improves pose prediction | Also improves feature extraction for better pose alignment |\n",
    "\n",
    "Both approaches refine pose predictions by integrating alignment feedback into training. Option 1 is easier to implement, while Option 2 may provide better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting?\n",
    "\"When the validation loss stops decreasing, while the training loss continues to decrease, your model starts overfitting. This means that the model starts sticking too much to the training set and looses its generalization power. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
