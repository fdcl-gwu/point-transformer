{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation of Point Transformer modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gazebo Data Structure\n",
    "05.02.2025 \n",
    "- 9988 clouds w/poses\n",
    "- gazebo_pc_record_full_12_42_labeled_1024\n",
    "    - clouds/\n",
    "    - poses/\n",
    "\n",
    "    Up/Downsampling used:\n",
    "    - clouds with < 1024: random duplication with slight gaussian noise\n",
    "    - clouds with > 1024: FPS (takes a long time)\n",
    "    - not removing any clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. train_pose.py\n",
    "Dataloader: SimNet or ScanNet\n",
    "\n",
    "#### 2. pointtransformer_pose.py\n",
    "```self.radius = 1.5```\n",
    "```input_dim```: change based on number of one-hot encoded classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform=False means our .ply files are already downsampled to 1024 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Translation-only prediction: _t.py\n",
    "- use train_pose_t.py, test_pose_t.py, and pointtransformer_pose_t.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training+Inference Process: \n",
    "0. ```$ conda activate point-transformer```\n",
    "1. train_pose.py\n",
    "    - pointtransformer_pose.py: contains PoseLoss()\n",
    "        - quaternion_to_rotation_matrix() assumes wxyz format as of 12.02.25\n",
    "2. test_pose.py\n",
    "    - manually specify best model saved during training to use for inference\n",
    "    - writes results as .json in XYZW format\n",
    "3. overlay_pose_estimate.py (MAC)\n",
    "    - reads from json and for each cloud file we used in training set (which the .json stores the pose prediction for), overlays the cloud on the stl model for visual evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation+Improvements:\n",
    "- Rotate cloud by prediction, then see how much they overlap (using some metric like average distance of model points to transformed cloud (AD/ADS))\n",
    "- Need more input (number of scans, labels, etc)?\n",
    "- Use the ground truth model somehow? i.e. dense ship cloud?\n",
    "- Experiment with MSG radius\n",
    "- Data augmentation beyond sensor pose variation\n",
    "- Examine what the Point Transformer portion learns. How is it representing scans?\n",
    "- Study how point distribution affects prediction (stern deck is denser than port, for eg.)\n",
    "- Try maybe an ICP approach for refinement?\n",
    "    - For this, first plot the input cloud in the space that the model reads it in. The idea is we want to see whether the raw clouds are \"perturbed\" enough by default, (since they're in the sensor frame), or whether the model actually does learn most of the rotation in the SE3 space.\n",
    "- Big IDEA for now: review how the input scans are represneted to the model, how+what does it learn about the scan that lets it be roughly transformed \"correctly\" to the ship in pyvista.\n",
    "- Try this with the 4.0 val error model. How much do more epochs improve the predictions?\n",
    "\n",
    "#### Initial Observations:\n",
    "- Is translation is struggling more than rotation? (either too low or too high)\n",
    "- 008054.txt is quite off in the yaw direction\n",
    "\n",
    "#### 13.02:\n",
    "- [x] Predict position first\n",
    "    - Don't forget about unit sphere scaling + centroid and how that affects it!\n",
    "- [ ] read more about decoupling rot and trans. does this allow the model to learn each better? i.e. would I try to learn rotation around origin? This again fits into the unit sphere situation i have right now.\n",
    "- [ ] Even distribution of scans in dataset (i.e. reduce bias towards close scans)\n",
    "    - Also means limit range of scan (i.e max furthest point from sensor)\n",
    "- [ ] visualize top K \n",
    "- [x] quickly NEXT, train by concatenating scale, centroid?\n",
    "    - no real impact on result\n",
    "- [ ] try quaternion loss\n",
    "- [ ] NEXT, try training without unit-sphere scaling (just centroid)\n",
    "    - might need to experiment with adjusting radii if not unit-sphere, or concatenating global scale? think...\n",
    "- [x] try with trained data (not much of a difference)\n",
    "- [ ] use ICP (geometric registration problem)? after transformer gets approximate pose\n",
    "    - \"In order to improve the registration perfor-\n",
    "mance, features on point clouds are also introduced for\n",
    "matching. \"\n",
    "- [ ] loss function predicting angle-axis, but does this cause issue with conversion?\n",
    "    - try different loss function (see GDR-Net), other parameterization besides angle axis\n",
    "- [ ] then try keypoint prediction\n",
    "    - plot the predicted keypoints after unit sphere scaling applied to them to ensure \n",
    "\n",
    "Question: should validation be used durign training and fed back? Or just printed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUATERNION FORMAT: WXYZ or XYZW: 12.02.25\n",
    "- gt stored as WXYZ quaterion\n",
    "- model learns angle-axis\n",
    "    - within loss function, its converted to rotation matrix to compare with ground truth (also converted to matrix)\n",
    "- NOTE: during inference (test_pose.py), we write pred_quat as XYZW to results.json!!\n",
    "    -  now writes ground truth gt_quat to be XYZW to results.json\n",
    "- on Mac, in overlay_pose_estimate.py, reads from results.json which has all XYZW format.\n",
    "\n",
    "<!-- TODO: ground truth should be as XYZW too -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "- Add plotting information to thesis document. Explain how i'm evaluating the results.\n",
    "\n",
    "- Tried directly regressing pose (translation resitual), but still not perfect. Before working with more/better dataset and dataset fine tuning, keep it how it is, and try\n",
    "\n",
    "    a) \n",
    "\n",
    "        - TRY THIS:Once predicting R,t, apply it to cloud and compute distance to gound truth model. use this as part of the loss (where alignment has lowest loss)\n",
    "        - this way the model also can compare the actual cloud error vs just the rotation error. Since we keep model in unnormalized space, this may be a work around.\n",
    "    b)\n",
    "\n",
    "        - TRY THIS: use PNP algorithm for predicting key points and then compute pose from those keypoints.\n",
    "\n",
    "    c)  - Still try to get centroid only (i.e. no scale) to work \n",
    "\n",
    "Once the \"Transformer\" is as good as possible, then:\n",
    "    a) Explore pose refinement (some ICP with ground truth ship model (since we have it for CAD and real ship))\n",
    "\n",
    "- keep everyting in normalized for training, but maybe give it some information in addition to this info?? Just appending before pose head didn't do anything.\n",
    "    - IDEA: can we make the \"label\" be the scale? So each scan cloud has a 4th entry which is the scale computed for that cloud? But then again, that's the same issue as not having a normalized input vector to begin with (i.e. all numbers are [-1,1] but 4th is large)\n",
    "- Clip scan cloud ranges, and experiment with query_ball_point radius (in case < nsample nearest points are found) \n",
    "- Think what ways we can use the Point Transformer output? It's basically just a good cloud feature representation.\n",
    "- Explore pose refinement (some ICP with ground truth ship model (since we have it for CAD and real ship))\n",
    "\n",
    "- ONLY THEN: try real data\n",
    "    - will allow model to learn with real noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Approach - Feb 21\n",
    "1. Try different ICP models between scan cloud and ground truth point cloud at inference time.\n",
    "    - [x] GICP (Open3D)\n",
    "    - [ ] Go-ICP (global one)\n",
    "    - [ ] small_gicp\n",
    "2. Incorporate alignment error into model training architecture \n",
    "    - [ ] Apply R,t to cloud and compute alignment error: (_D. 3D Rotation Regression. PoseCNN paper_)\n",
    "        - [ ] error using point-to-point. Compute error btw both clouds and add it to the loss function.\n",
    "        - [ ] error comparing scan and global feature. Compute feature vector of transformed cloud and ground truth cloud, where alignment of the two have lower loss (key: what does the \"feature difference\" look like?).\n",
    "    - [ ] If not, try PointNetLK to iteratively refine R,t instead of predicting PoseLoss\n",
    "    - See Deep Closest Point, PCRNet, Go-ICP, DeepVCP/DeepCLR,\n",
    "    - Explore whether point-to-point or feature-to-feature is better (since we're not registering identical scans)\n",
    "3. Other ideas:\n",
    "    - __Resolve scale problem:__\n",
    "        - concatenate scale to each point in cloud. Challenge: not normalized either\n",
    "            - Sol: normalize scale dataset wide? Yes, but save the mean-std scales computed on the trianing dataset to apply to the scan at inference.\n",
    "        - make scale prediction MLP in additon to R,t (_Disentangled 6D Pose Loss.GDR-Net paper_)\n",
    "            - doesn't force the feature extraction layers to also learn scale, since its all tied into the final estimate.\n",
    "        - maybe alignment/chamfer solves it. Challenge: may be costly to compute.\n",
    "\n",
    "\n",
    "The BIG IDEA: we keep Point Transformer in unit sphere space since it can learn features fairly well, but then we a) improve refinement to be robust (deep learning registration options) and/or b) let the model learn with alignment error (in some combination of the two). __Features are more robust than points__\n",
    "\n",
    "#### QUICK IDEAS \n",
    "25.Feb:\n",
    "- add scale to 4th col point feature (with avg normalized across dataset)\n",
    "- try alignment loss function (option 1, option 2)\n",
    "- \"intelligent\" ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - Point Transformer\n",
    "\n",
    "#### NOTE: 16.02-19.02 Using squared L2 norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ BEST RUN ------------------------\n",
    "_2025-02-20_21-50_:\n",
    "```\n",
    "{'num_points': 1024, 'batch_size': 11, 'use_labels': False, 'optimizer': 'RangerVA', 'lr': 0.001, 'decay_rate': 1e-06, 'epochs': 60, 'dropout': 0.4, 'M': 4, 'K': 64, 'd_m': 512, 'alpha': 10, 'beta': 1, 'radius_max_points': 32, 'radius': 0.2, 'unit_sphere': True}\n",
    "\n",
    "Using L2 Norm, 12 Dataset\n",
    "```\n",
    "#### ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 16.02.25 (1): add scale term to pose head input (_t only)\n",
    "NOTE!!: initially forgot to multiply `predicted_translation` by `scale`\n",
    "\n",
    "Result dir: pose_estimation/2025-02-16_15-59\n",
    "Observation: loss not much better original approach, \n",
    "\n",
    "Changes: \n",
    "\n",
    "- pointtransformer_pose_t.py\n",
    "    ```\n",
    "    self.translation_mlp = nn.Sequential(\n",
    "    nn.Linear(dim_flatten+1, 512),\n",
    "    ```\n",
    "\n",
    "- within PointTransformer.forward():\n",
    "    ```\n",
    "    # Flatten the feature vector for MLP heads\n",
    "    global_features = torch.flatten(embedding, start_dim=1)  # [B, dim_flatten]\n",
    "    if scale.dim() == 1:\n",
    "        scale = scale.unsqueeze(1)  # Expands shape from [B] to [B, 1]\n",
    "    # Predict translation residual (normalized space)\n",
    "    translation_input = torch.cat([global_features, scale], dim=1)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 16.02.25 (2): Hybrid - Only Centroid, No Scale (INCOMPLETE)\n",
    "\n",
    "\n",
    "Set config.unit_sphere = False. The issue currently is that conv2d dimensions for the ball query search aren't working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (1): Regular training run\n",
    "Details: \n",
    "- ```predicted_translation = predicted_translation_residual * scale + centroid``` applied to model output in PoseLoss()\n",
    "- 'alpha': 20, 'beta': 2, 'radius_max_points': 32, 'radius': 0.2\n",
    "\n",
    "Result dir: 2025-02-19_15-10\n",
    "\n",
    "Observation: decent R, t prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (2): Learning absolute translation (not residual)\n",
    "Details: ```predicted_translation = predicted_translation_residual```\n",
    "\n",
    "Result dir: 2025-02-19_15-20\n",
    "\n",
    "Observation: terrible translation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (3): Regular training run\n",
    "Details: 'radius_max_points': 32, 'radius': 0.1,\n",
    "\n",
    "Result dir: 2025-02-19_20-57 (100 epoch: 2025-02-19_23-03, 'radius': 0.2)\n",
    "\n",
    "Observation: regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 19.02.25 (4): Regular training run SimNet2\n",
    "Details: same as 2025-02-19_20-57, except on SimNet2 (i.e. 15.41 dataset)\n",
    "\n",
    "Result dir: 2025-02-19_21-27 (100 epoch: 2025-02-19_23-05, 'radius': 0.2)\n",
    "\n",
    "Observation: better than fewer epochs, but still not much different... a bit worse visually than 12.42 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 21.02.25 (1/2): L2 Loss (not squared L2 Loss)\n",
    "Details: corrected L2 Loss with two different learning rates. Note, training uses lr scheduler.\n",
    "\n",
    "Result dir: 2025-02-20_21-49 (for 'lr': 0.0005), 2025-02-20_21-50 (for 'lr': 0.001)\n",
    "\n",
    "Observation: translation a bit better, but rotation not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 21.02.25 (3):'radius_max_points': 16,'radius': 0.1\n",
    "Details: tried using default params as specified in papers\n",
    "\n",
    "Result dir: 2025-02-21_12-33\n",
    "\n",
    "Observation: quite a lot worse than 32,'radius': 0.2. Likely a property of the clouds that are captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 25.02.25 (1):'lr scheduler step_size': 15, dataset = merged 12,15 (SimNet15)\n",
    "Details: reduced lr scheduler step size\n",
    "\n",
    "Dataset: using merged datasets `gazebo_pc_record_full_12_42_1024` and `gazebo_pc_record_full_15_41_1024`.\n",
    "\n",
    "Result dir: 2025-02-25_14-01 (on MAC, 02-25_16-35)\n",
    "\n",
    "Observation: similar to best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 25.02.25 (2):Scale MLP layer\n",
    "Dataset: 12\n",
    "\n",
    "Details: lr scheduler step size=15, added scale_mlp(),  and model now predicts scale \n",
    "\n",
    "Changes:\n",
    "- In PoseLoss()\n",
    "    ```\n",
    "        loss_scale = F.mse_loss(pred_scale, gt_scale)\n",
    "        total_loss = (self.alpha * loss_t) + (self.beta * loss_r) + loss_scale\n",
    "    ```\n",
    "\n",
    "- in PointTransformer.forward()\n",
    "    ```\n",
    "        predicted_scale = self.scale_mlp(global_features)\n",
    "        scale = scale.unsqueeze(1)  # Expands shape from (B,) to (B,1)\n",
    "        predicted_translation = predicted_translation_residual * predicted_scale + centroid\n",
    "\n",
    "    ```\n",
    "\n",
    "Result dir: 2025-02-25_13-35 (on MAC, 02-25_16-00)\n",
    "\n",
    "Observation: similar to best\n",
    "TODO: try with normalized scale instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 27.02.25 (1): Larger+more MLP layers\n",
    "Dataset: 12\n",
    "\n",
    "Details: lr scheduler step size=15,  made pose MLP heads start at 4096, but shoudl also try 1024\n",
    "\n",
    "Result dir: \n",
    "- 4096:\n",
    "    - 2025-02-27_12-06\n",
    "- 1024:\n",
    "    - 2025-02-27_12-11 (on MAC: 2025-02-27_13-32)\n",
    "\n",
    "Observation: slightly worse than 16-00 and 16-35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (1): New Dataset (far, normal)\n",
    "Dataset: SimNet_far\n",
    "\n",
    "Details: lidar samples=512, using truncated normal dist.\n",
    "\n",
    "Result dir: \n",
    "- 2025-03-02_14-49\n",
    "\n",
    "Observation: No real difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (2): New Dataset (close, normal)\n",
    "Dataset: SimNet_close\n",
    "\n",
    "Details: lidar samples=512, using truncated normal dist. reduced theta for C points so we don't cover side of ship as much. \n",
    "\n",
    "Result dir: \n",
    "- 2025-03-03_12-02 (60 epochs) (on MAC: 2025-03-03_13-48)\n",
    "- 2025-03-03_14-21 (120 epochs) (on MAC: 2025-03-03_19-35)\n",
    "\n",
    "Observation: \n",
    "- 60 epochs: roll usually better, since flight deck is large flat surface (easy to align)\n",
    "    - GICP using closest point, which doesn't help translation alignment (14/500)\n",
    "    - bad one: 13/500\n",
    "- 120 epochs: marginally better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 03.03.25 (3): Merged Dataset (close+far, normal)\n",
    "Dataset: SimNet_close + SimNet_far\n",
    "\n",
    "Details:  \n",
    "\n",
    "Result dir: \n",
    "- 2025-03-03_14-23 (on MAC: 2025-03-04_09-54)\n",
    "\n",
    "Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test List - GICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------ BEST RUN ------------------------\n",
    "\n",
    "#### Using _2025-02-20_21-50_ (i.e. 2025-02-21_12-22 on MAC after running test_pose.py):\n",
    "- Uses TransformationEstimationForGeneralizedICP with max_iteration=1\n",
    "\n",
    "- Under `model_ouput/2025-02-21_12-22`\n",
    "    - fitness_rmse_15_fit_rmse.txt and refined_results fitness_rmse_15_fit_rmse.json\n",
    "#### ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation Refinement: Two Training Strategies\n",
    "\n",
    "### Option 1: Point-Level Alignment (Chamfer Distance)\n",
    "#### Overview:\n",
    "- After predicting the pose (R, t), apply it to transform the scan cloud.\n",
    "- Compute alignment loss between the transformed scan and the ground truth ship cloud.\n",
    "- Loss penalizes misalignment, improving pose predictions over training.\n",
    "\n",
    "#### Steps:\n",
    "1. Extract scan features using Point Transformer.\n",
    "2. Predict pose (R, t) using Pose MLP.\n",
    "3. Apply the predicted transformation to the scan cloud.\n",
    "4. Compute Chamfer Distance between transformed scan and ship cloud.\n",
    "5. Backpropagate alignment loss to refine pose predictions.\n",
    "\n",
    "#### Loss Function:\n",
    "- Pose loss (direct R, t error).\n",
    "- Chamfer Distance loss (point-wise alignment error).\n",
    "- Total loss combines both.\n",
    "\n",
    "#### Key Benefits:\n",
    "- Simple to implement and integrates well into direct pose regression.\n",
    "- Improves translation prediction, especially with unit scaling issues.\n",
    "- Does not require a learned feature vector for the ship model.\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Feature-Based Alignment (Global Feature Comparison)\n",
    "#### Overview:\n",
    "- Instead of point-wise alignment, compare feature vectors between the transformed scan and the ship.\n",
    "- Forces the network to learn feature spaces where aligned scans and ships are similar.\n",
    "\n",
    "#### Steps:\n",
    "1. Extract scan features using Point Transformer.\n",
    "2. Predict pose (R, t) using Pose MLP.\n",
    "3. Apply the predicted transformation to the scan cloud.\n",
    "4. Extract a global feature vector from the transformed scan.\n",
    "5. Extract a fixed global feature vector for the ship cloud.\n",
    "6. Compute alignment loss based on feature similarity.\n",
    "7. Backpropagate to refine both feature learning and pose prediction.\n",
    "\n",
    "#### Loss Function:\n",
    "- Pose loss (direct R, t error).\n",
    "- Feature distance loss (L2 difference between transformed scan and ship features).\n",
    "- Total loss combines both.\n",
    "\n",
    "#### Key Benefits:\n",
    "- Helps the model generalize better by enforcing feature-based alignment.\n",
    "- Encourages the Point Transformer to extract features that naturally align after transformation.\n",
    "- Reduces the need for iterative refinement at inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Differences\n",
    "| Feature | Option 1: Chamfer Distance | Option 2: Feature Alignment |\n",
    "|---------|---------------------------|----------------------------|\n",
    "| **Alignment Type** | Point-wise (geometry-based) | Feature-space (embedding-based) |\n",
    "| **Ship Model Representation** | Fixed point cloud (no learned features) | Learned feature representation |\n",
    "| **Loss Supervision** | Chamfer Distance between point clouds | L2 difference between feature vectors |\n",
    "| **Computational Complexity** | Moderate | Higher due to feature extraction for ship model |\n",
    "| **Training Effect** | Directly improves pose prediction | Also improves feature extraction for better pose alignment |\n",
    "\n",
    "Both approaches refine pose predictions by integrating alignment feedback into training. Option 1 is easier to implement, while Option 2 may provide better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting?\n",
    "\"When the validation loss stops decreasing, while the training loss continues to decrease, your model starts overfitting. This means that the model starts sticking too much to the training set and looses its generalization power. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
